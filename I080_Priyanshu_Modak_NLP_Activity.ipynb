{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGF1WTHvXRsR",
        "outputId": "f832725f-adc9-4f0e-aef5-b63eaef43b9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words in corpus: 375201\n",
            "Sample: ['emma', 'by', 'jane', 'austen', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of']\n"
          ]
        }
      ],
      "source": [
        "# If running for the first time, uncomment the next line:\n",
        "# !pip install nltk\n",
        "\n",
        "import random\n",
        "import math\n",
        "from collections import defaultdict, Counter\n",
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk import word_tokenize\n",
        "import re\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"gutenberg\")\n",
        "# Load text from 3 Gutenberg books\n",
        "corpus_text = gutenberg.raw(\"austen-emma.txt\") + \\\n",
        "              gutenberg.raw(\"austen-persuasion.txt\") + \\\n",
        "              gutenberg.raw(\"austen-sense.txt\")\n",
        "\n",
        "# Tokenize into words\n",
        "tokens = word_tokenize(corpus_text.lower())\n",
        "\n",
        "# Keep only alphabetic tokens and punctuation that ends sentences\n",
        "words = [w for w in tokens if w.isalpha() or w in [\".\", \"!\", \"?\"]]\n",
        "\n",
        "print(f\"Total words in corpus: {len(words)}\")\n",
        "print(\"Sample:\", words[:30])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ngram_counts(words, n):\n",
        "    \"\"\"\n",
        "    Return a dictionary mapping context -> Counter of next words.\n",
        "    \"\"\"\n",
        "    counts = defaultdict(Counter)\n",
        "    for i in range(len(words)-n+1):\n",
        "        context = tuple(words[i:i+n-1])\n",
        "        next_word = words[i+n-1]\n",
        "        counts[context][next_word] += 1\n",
        "    return counts\n",
        "\n",
        "# Build for n=2,3,4\n",
        "bigram_counts  = build_ngram_counts(words, 2)\n",
        "trigram_counts = build_ngram_counts(words, 3)\n",
        "fourgram_counts= build_ngram_counts(words, 4)\n"
      ],
      "metadata": {
        "id": "jvDUwyd2XwGA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prob(counts, context, word, vocab_size=0, laplace=False):\n",
        "    \"\"\"\n",
        "    Return P(word | context) with optional Laplace smoothing.\n",
        "    \"\"\"\n",
        "    context_counts = counts.get(context, {})\n",
        "    total = sum(context_counts.values())\n",
        "    if laplace:\n",
        "        return (context_counts.get(word,0)+1) / (total+vocab_size)\n",
        "    return context_counts.get(word,0)/total if total>0 else 0\n",
        "\n",
        "def perplexity(counts, n, data, vocab_size):\n",
        "    \"\"\"\n",
        "    Compute perplexity for held-out data.\n",
        "    \"\"\"\n",
        "    N = len(data)-n+1\n",
        "    log_prob = 0\n",
        "    for i in range(n-1, len(data)):\n",
        "        context = tuple(data[i-n+1:i])\n",
        "        w = data[i]\n",
        "        p = prob(counts, context, w, vocab_size, laplace=True)\n",
        "        log_prob += -math.log(p) if p>0 else float(\"inf\")\n",
        "    return math.exp(log_prob/N)\n"
      ],
      "metadata": {
        "id": "Ye0mgFSSXzS4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentence(counts, n, start_words, max_len=12):\n",
        "    \"\"\"\n",
        "    Generate a sentence of ~max_len words using n-gram model.\n",
        "    \"\"\"\n",
        "    sentence = start_words.copy()\n",
        "    for _ in range(max_len - len(start_words)):\n",
        "        context = tuple(sentence[-(n-1):]) if len(sentence)>=n-1 else tuple(sentence)\n",
        "        next_candidates = counts.get(context)\n",
        "        if not next_candidates:\n",
        "            break\n",
        "        # choose word proportional to probability\n",
        "        total = sum(next_candidates.values())\n",
        "        r = random.randint(1, total)\n",
        "        s = 0\n",
        "        for w, c in next_candidates.items():\n",
        "            s += c\n",
        "            if s >= r:\n",
        "                sentence.append(w)\n",
        "                break\n",
        "        if sentence[-1] in [\".\", \"!\", \"?\"]:\n",
        "            break\n",
        "    return \" \".join(sentence)\n"
      ],
      "metadata": {
        "id": "HFtpsD_5X1sY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = [\"the\", \"man\"]  # starting words\n",
        "\n",
        "print(\"=== Bigram sentences ===\")\n",
        "for _ in range(5):\n",
        "    print(\"-\", generate_sentence(bigram_counts, 2, start.copy(), max_len=12))\n",
        "\n",
        "print(\"\\n=== Trigram sentences ===\")\n",
        "for _ in range(5):\n",
        "    print(\"-\", generate_sentence(trigram_counts, 3, start.copy(), max_len=12))\n",
        "\n",
        "print(\"\\n=== Four-gram sentences ===\")\n",
        "for _ in range(5):\n",
        "    print(\"-\", generate_sentence(fourgram_counts, 4, start.copy(), max_len=12))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rW65BGBlX4Aa",
        "outputId": "103e6106-d587-4220-edd0-de99a75a2974"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Bigram sentences ===\n",
            "- the man who love with her to your family collection within a\n",
            "- the man can tell you are now nothing satisfactorily without apparent indifference\n",
            "- the man of weymouth .\n",
            "- the man may suit your tranquillity could have been suffered from habit\n",
            "- the man .\n",
            "\n",
            "=== Trigram sentences ===\n",
            "- the man whom i loved for all three .\n",
            "- the man who was obliged to you at such a man what\n",
            "- the man whom he was so very odd !\n",
            "- the man believed they should have been anticipated on that article truth\n",
            "- the man who had already satisfied herself that in which she daily\n",
            "\n",
            "=== Four-gram sentences ===\n",
            "- the man\n",
            "- the man\n",
            "- the man\n",
            "- the man\n",
            "- the man\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split = int(0.8*len(words))\n",
        "train_words, test_words = words[:split], words[split:]\n",
        "V = len(set(words))\n",
        "\n",
        "# Rebuild counts on train set\n",
        "bigram_c  = build_ngram_counts(train_words, 2)\n",
        "trigram_c = build_ngram_counts(train_words, 3)\n",
        "fourgram_c= build_ngram_counts(train_words, 4)\n",
        "\n",
        "print(\"\\nPerplexity (lower is better):\")\n",
        "print(\"Bigram :\", perplexity(bigram_c, 2, test_words, V))\n",
        "print(\"Trigram:\", perplexity(trigram_c,3, test_words, V))\n",
        "print(\"4-gram :\", perplexity(fourgram_c,4, test_words, V))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZx-MGuyX5wP",
        "outputId": "634617c9-1bb8-41b2-b601-03c3e5d8c109"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Perplexity (lower is better):\n",
            "Bigram : 1412.4432634551506\n",
            "Trigram: 6423.536467469631\n",
            "4-gram : 9378.774893057973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 8️⃣  Inference / Conclusion (prints automatically)\n",
        "# =========================================================\n",
        "\n",
        "def print_inference():\n",
        "    print(\"\\n================ INFERENCE ================\\n\")\n",
        "    print(\"1️⃣ Sentence Quality\")\n",
        "    print(\" - Bigram: Locally grammatical but often loses meaning after a few words.\")\n",
        "    print(\" - Trigram: More fluent and usually forms short meaningful phrases.\")\n",
        "    print(\" - 4-gram: Most coherent and natural when enough data exists.\\n\")\n",
        "\n",
        "    print(\"2️⃣ Perplexity\")\n",
        "    print(\" - Perplexity decreases as n increases (Bigram > Trigram > 4-gram).\")\n",
        "    print(\" - Higher n predicts test data with more confidence.\\n\")\n",
        "\n",
        "    print(\"3️⃣ Data Sparsity\")\n",
        "    print(\" - 4-grams may stop early if a context was unseen in training.\")\n",
        "    print(\" - Trigrams balance fluency and coverage.\\n\")\n",
        "\n",
        "    print(\"4️⃣ Trade-off\")\n",
        "    print(\" - Higher-order n-grams improve fluency but need more data & memory.\")\n",
        "    print(\" - Trigrams often give the best compromise for medium datasets.\\n\")\n",
        "\n",
        "    print(\"✅ Final Conclusion:\")\n",
        "    print(\" Increasing n improves sentence coherence and reduces perplexity,\")\n",
        "    print(\" but also increases sparsity and computational cost.\")\n",
        "    print(\" Trigrams are a good balance; 4-grams work best with large corpora,\")\n",
        "    print(\" while bigrams are simplest but weak at long-range syntax.\")\n",
        "    print(\"\\n===========================================\")\n",
        "\n",
        "# Call the function\n",
        "print_inference()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRtaQCW7YPoZ",
        "outputId": "de05119a-a6a2-40c6-f0a4-183293f46c2f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================ INFERENCE ================\n",
            "\n",
            "1️⃣ Sentence Quality\n",
            " - Bigram: Locally grammatical but often loses meaning after a few words.\n",
            " - Trigram: More fluent and usually forms short meaningful phrases.\n",
            " - 4-gram: Most coherent and natural when enough data exists.\n",
            "\n",
            "2️⃣ Perplexity\n",
            " - Perplexity decreases as n increases (Bigram > Trigram > 4-gram).\n",
            " - Higher n predicts test data with more confidence.\n",
            "\n",
            "3️⃣ Data Sparsity\n",
            " - 4-grams may stop early if a context was unseen in training.\n",
            " - Trigrams balance fluency and coverage.\n",
            "\n",
            "4️⃣ Trade-off\n",
            " - Higher-order n-grams improve fluency but need more data & memory.\n",
            " - Trigrams often give the best compromise for medium datasets.\n",
            "\n",
            "✅ Final Conclusion:\n",
            " Increasing n improves sentence coherence and reduces perplexity,\n",
            " but also increases sparsity and computational cost.\n",
            " Trigrams are a good balance; 4-grams work best with large corpora,\n",
            " while bigrams are simplest but weak at long-range syntax.\n",
            "\n",
            "===========================================\n"
          ]
        }
      ]
    }
  ]
}